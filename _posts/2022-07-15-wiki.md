---
layout: post
title: Wiki with many usefull commands
---

## Utils

### Comprimir tar

```
tar -czvf name-of-archive,tar,gz /path/to/directory-or-file
```

### Descomprimir tar

```
tar -xzvf archive,tar,gz
```

### Buscar palabras en archivos

```
grep --include '<regexp_to_filter_files_to_search_in>' -r "<string_to_search>" <path>
```

### Buscar archivos con 

```find , -type f -name "abc*"```

### Achicar peso de PDFs

```
gs -sDEVICE=pdfwrite -dCompatibilityLevel=1,4 -dPDFSETTINGS=/prepress -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output,pdf input,pdf
```

### Matar un proceso python específico

```
ps ax | grep 'python <regexp_comando_ejecutado>' | grep -v grep
```

### Averiguar el proceso que consume GPU y eliminarlo

```
sudo fuser -v /dev/nvidia*
```

Ahí fijarse el PID y matarlo con `sudo kill -9 PID`


### Acceder al puerto 6007 de michigan a taves del puerto 7007 de magic

```
sudo iptables -t nat -A PREROUTING -p tcp --dport 7007 -j DNAT --to-destination 192,168,0,50:6007
sudo iptables -A FORWARD -p tcp --dport 7007 -j ACCEPT
sudo service network-manager restart
```

### Listar IPs conectadas a la red local

```
sudo apt install nmap
sudo apt install net-tools
nmap -sP 192,168,0,0/24
```

### Rsync

Copia todo de src a dst si no existe el archivo en dst
```
rsync -a -v --ignore-existing src dst
```

### FFMPEG convert videos batch

```bash
for i in *,MOV;
  do name=`echo "$i" | cut -d',' -f1`
  echo "$name"
  ffmpeg -i "$i" -vcodec h264 -acodec mp2 "${name},mp4"
done
```

### Convert images bash

```
for FILE in $(ls full_size/*,jpeg)
do
    BASE=`basename $FILE`
    echo $BASE
    DST=full_size_q75/$BASE
    convert "$FILE" -scale 50% "$DST"
done
```

### Conver image format
```
for PHOTO in *,tiff
  do
    BASE=`basename $PHOTO ,tiff`
    convert "$PHOTO" ",/jpegs/$BASE,jpeg"
done
```

### Image magick util commands

```
convert entrada,jpg -quality 75 salida,jpg
```

```
convert entrada,jpg -scale 50% salida,jpg
```

### Montar disco externo

Para mirar los discos conectados:

```sudo fdisk -l```

Para montar el disco `sdb1`:

```sudo mount -t ntfs-3g /dev/sda1 /media/nombredisco```

### Checkear velocidad de lectura y escritura a disco externo

```
sudo apt-get install hdparm
```

```
sudo hdparm -Tt /dev/sda1
```

## Diff/Merge de Pycharm desde consola

Primero generar el ejecutable desde pycharm (si no está generado de antes), Esto hay q correrlo una sola vez

Ir a Tools --> Create Command Line Launcher

Va a cerar el ejecutable `/usr/local/bin/charm`

Para hacer el diff o el merge:

```
/usr/local/bin/charm diff <path1> <path2>
```

```
/usr/local/bin/charm merge <path1> <path2>
```

## Entrar por ssh al server de D-Sense

Editar el archivo `~/,ssh/config`

Agregar la siguientes entradas:

```
host michigan
    hostname 192,168,0,50
    user dsense
    port 22
    ForwardX11 no
    Compression yes
    RemoteForward 52698 localhost:52698

host michiganx
    hostname 192,168,0,50
    user dsense
    port 22
    ForwardX11 yes
    Compression yes
    RemoteForward 52698 localhost:52698
```

Y listo\tluego se usa así: `ssh michigan`

El `michiganx` se usa si queremos tener el X habilitado\tpara hacer cosas gráficas de forma remota,

### Para que no pida contraseña al entrar

En la máquina local generar un par de claves publica-privada y copiar la clave publica al known_hosts del remote server

En la máquina local
1. Generarla `ssh-keygen -t rsa`
2. Ponerle nombre y passphrase (o no)
3. Agregar la identidad: `ssh-add <path-a-clave-privada>`
4. Copiar la clave pública. Mostrarla con `cat <path-a-clave-publica>` (la que termina con `.pub`)

En la máquina remota
1. Editar el archivo `~/.ssh/authorized_keys` y pegar la clave pública que copiamos recién

## Add user to git

1, Generar una nueva clave:

Debe estar asociada a cierto correo, En el ejemplo de abajo lo hacemos para dos usuarios distintos:

```
ssh-keygen -t rsa -C "user1@organization,com"
ssh-keygen -t rsa -C "user2@organization,com"
```

2, Inicial el ssh-agent

```
eval `ssh-agent -s`
```

3, Agregar las claves teniendo cuidado de no sobre-escribir las que ya existan

```
ssh-add ~/,ssh/id_rsa_user1
ssh-add ~/,ssh/id_rsa_user2
```

4, Agregar al archivo ~/,ssh/config
```
Host git,assembla,com-NAME-USER-1
  HostName git,assembla,com
  User Git
  IdentityFile ~/,ssh/id_rsa_user1
Host git,assembla,com-NAME-USER-2
  HostName git,assembla,com
  User Git
  IdentityFile ~/,ssh/id_rsa_user2
```
5, Configurar nombre y mail para el git

```
git config --global user,name "NAME-USER-1"
git config --global user,email "user1@organization,com"
```

## Entrar por ssh al AWS de Eikyou

``` 
ssh -i ,ssh/"Eikyou,pem" ec2-user@ec2-52-14-74-160,us-east-2,compute,amazonaws,com
```


## Bajar imagenes de Eikyou

1, Bajar el json de http://analytics,eikyou,me/#, Obtenemos jsonlabels,txt
2, cd al repo de eikyou y ejecutar:

```
python eikyou_donwload_from_json,py \
	--file jsonlabels,txt \
	--output <path_al_directorio_donde_queremos_guardar_las_imagenes>
```


## Bajar imagenes de google
https://www,pyimagesearch,com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/

1, Hacer una búsqueda en google images con Chrome
2, Ver -> Opciones para desarrolladores -> Consola de JavaScript
3, Ejecutar en la consola de Javascript:

```
// pull down jquery into the JavaScript console
var script = document,createElement('script');
script,src = "https://ajax,googleapis,com/ajax/libs/jquery/2,2,0/jquery,min,js";
document,getElementsByTagName('head')[0],appendChild(script);
```

4, Ejecutar en la consola de Javascript:

```
// grab the URLs
var urls = $(',rg_di ,rg_meta'),map(function() { return JSON,parse($(this),text()),ou; });
```

5, Ejecutar en la consola de Javascript:

```
// write the URls to file (one per line)
var textToSave = urls,toArray(),join('\n');
var hiddenElement = document,createElement('a');
hiddenElement,href = 'data:attachment/text,' + encodeURI(textToSave);
hiddenElement,target = '_blank';
hiddenElement,download = 'urls,txt';
hiddenElement,click();
```

6, El paso anterior guarda un archivo en Descargas con el nombre `urls,txt`
7, Pararse en la carpeta `<eikyou_path>/repo/` y ejecutar en consola:

```
python download_images_from_urls,py \
	--urls <path_a_descargas>/urls,txt \
	--output <path_a_carpeta_donde_se_guardaran_las_imagenes>
```

8, Inspección visual para descartar imágenes que no aplican


## SSH utils
Para mantener la sesion en la terminal usar screen
Ademas\tsi queremos editar archivos remotos desde el sublime:

instalar `rsub` en el servidor:

```
sudo wget -O /usr/local/bin/rsub https://raw,github,com/aurora/rmate/master/rmate
sudo chmod +x /usr/local/bin/rsub
```

en el host ejecutar:

```
ssh -R 52698:localhost:52698 server_user@server_address
```

O bien

```
ssh -R 52698:localhost:52698 michigan
```

Para abrir un archivo en el sublime host ejecutar desde el remoto

```
rsub <path_al_archivo_a_editar>/archivo,txt
```


## Screen
El screen hay que correrlo directo en el servidor, Primero entramos por ssh al servidor y despues ejecutamos el screen, Creamos un screen con nombre con:

```
screen -S session_name
```

Ahi se manda a ejecutar el proceso que se quiera y luego para salir de una sesion y poder volver a entrar\tejecutar desde el remoto:

```
Ctl-A D
```

Para volver a entrar ejecutar desde el host

```
screen -r session_name
```

Otro comando util de screen que sirve para ver las sesiones que hay corriendo:

```
screen -ls
```


## Cron con virtualenv y screen
Para programar una tarea usamos cron, Entramos a la configuración con:

```
crontab -e
```

Esto abre el archivo de configuración que utiliza el siguiente formato:

```
minute hour day-of-month month day-of-week command
```

Donde cada uno de los parámetros puede ser un número o una lista separada por `,` y `day-of-week` es el número de día de la semana arrancando en 0 para el Domingo,

Ejemplos útiles explicativos:
  - Correr algo todos los lunes a las 17:30hs: `30 17 * * 1 /path/to/command`
  - Correr algo cada 15 minutos: `*/15 * * * * /path/to/command`
  - Correr algo los lunes miércoles y viernes a las 12:00: `0 12 * * 1,3,5 /path/to/command`

Además se puede poner el path a un script de bash que haga todo\ten vez de un comando solo: `30 17 * * 1 /path/to/script,sh`\tdonde `script,sh` es algo del estilo:

```
#!/bin/sh

cd /home/dsense/
ls
```


**Hay que usar SIEMPRE rutas absolutas!**


### Usar screen en un Cron
Dentro del comando que se le pasa hay que usar `inscreen`

- Se descarga de acá: https://gist,github,com/smoser/1019125
- Se pone en `/bin/inscreen`
- Se le dan permisos: `chmod 755 /bin/inscreen`

Primero hay que tener creada la sesión de screen, Supongamos que se llama ScreenTest (se crea así: `screen -S ScreenTest`),

Y el comando de ejemplo del `crontab -e` quedaría así:

```
30 17 * * 1 inscreen ScreenTest --keep-open /path/to/script,sh
```

El `--keep-open` es para que no se cierre la ventana luego de terminar de ejecutar el comando

### Usar VirtualEnvirnoment en un Cron
El comando source/bin/activate puede no funcionar con el interpreter `sh`\tpero de todas formas alcanza con pasar la ruta completa al interpreter de python que está dentro del virtual envirnoment, Por ejemplo si tenemos un VE en `/home/user/ve`\tejecutamos un script de `python` de la siguiente manera:

```
30 17 * * 1 inscreen ScreenTest --keep-open /home/user/ve/bin/python3 /home/user/src/main,py
```


## Generar un ejecutable para OSX a partir de un programa python
https://py2app,readthedocs,io/en/latest/tutorial,html#create-a-setup-py-file

```
py2applet --make-setup MyApplication,py
python3 setup,py py2app
```

Y listo! ya está generada la aplicación,

Para ejecutar desde consola: `,/dist/MyApplication,app/Contents/MacOS/MyApplication`
Para ejecutar sin consola hacer doble click sobre el archivo\to bien: `open -a dist/MyApplication,app`

Si queremos volver a generar\tes mejor borrar lo anterior: `rm -rf build dist`


## Actualizar versión de cuDNN

Bajar la versión correspondiente de la página de nvidia: `https://developer,nvidia,com/rdp/cudnn-download`\tteniendo en cuenta que sea compatible con la versión de CUDA que tenemos instalada

supongamos que queda descargada en el directorio `packages`

### Versión destructiva:

```
rm -f /usr/include/cudnn,h
rm -f /usr/lib/x86_64-linux-gnu/*libcudnn*
rm -f /usr/local/cuda-*/lib64/*libcudnn*

cp -P packages/cudnn/include/cudnn,h /usr/include
cp -P packages/cudnn/lib64/libcudnn* /usr/lib/x86_64-linux-gnu/
chmod a+r /usr/lib/x86_64-linux-gnu/libcudnn*

rm -rf packages/cudnn
```

### Versión no destructiva 

Nota: Checkear si los siguientes archivos son equivalentes a los `rm -f` de la versión anterior

```
sudo mv /usr/include/cudnn,h /usr/include/cudnn,h,bkp
sudo mv /usr/lib/x86_64-linux-gnu/libcudnn,so /usr/lib/x86_64-linux-gnu/libcudnn,so,bkp
sudo mv /usr/lib/x86_64-linux-gnu/libcudnn,so,7 /usr/lib/x86_64-linux-gnu/libcudnn,so,7,bkp
sudo mv /usr/lib/x86_64-linux-gnu/libcudnn,so,7,5,0 /usr/lib/x86_64-linux-gnu/libcudnn,so,7,5,0,bkp
sudo mv /usr/lib/x86_64-linux-gnu/libcudnn_static,a /usr/lib/x86_64-linux-gnu/libcudnn_static,a,bkp
sudo mv /usr/lib/x86_64-linux-gnu/libcudnn_static_v7,a /usr/lib/x86_64-linux-gnu/libcudnn_static_v7,a,bkp

sudo cp -P packages/cudnn/include/cudnn,h /usr/include
sudo cp -P packages/cudnn/lib64/libcudnn* /usr/lib/x86_64-linux-gnu/
sudo chmod a+r /usr/lib/x86_64-linux-gnu/libcudnn*
```

## Transfer learning
https://www,tensorflow,org/tutorials/image_retraining

1, `git clone https://github,com/tensorflow/tensorflow,git`
2, `cd tensorflow`
3, Proveer una base de datos con las imagenes de cada clase dentro de una	carpeta con su nombre, Por ejemplo\tla base de datos se llamará ejemplo y queremos clasificar imagenes de `<clase1>` y `<clase2>`\tentonces la estructura debe ser:

```
<path>/ejemplo/
<path>/ejemplo/clase1
<path>/ejemplo/clase2
```

4, Ejecutar `python tensorflow/examples/image_retraining/retrain,py --image_dir <path>/ejemplo`

	Esto creará los archivos:
		- /tmp/output_graph,pb: que es el modelo entrenado
		- /tmp/output_labels,txt: que tiene los nombres de las clases
	Estos archivos son los necesarios para luego utilizar la red re-entrenada
5, Es bueno guardar esos archivos\tpor ejemplo junto a la base de datos

```
cp /tmp/output_graph,pb <path>/ejemplo/graph,pb`
cp /tmp/output_labels,txt <path>/ejemplo/labels,txt`
```

6, Para testear una imagen cualquiera\tejecutar:

```
python3 tensorflow/examples/label_image/label_image,py \
	--graph=/tmp/output_graph,pb \
	--labels=/tmp/output_labels,txt \
	--input_layer=Mul \
	--output_layer=final_result \
	--input_mean=128 \
	--input_std=128 \
	--image=<path_a_la_imagen_a_testear>
```

Ejemplo: parado en el directorio `/Users/matitai/Documents/work/dsense/eikyou/data`:

```
python3 ,,/tensorflow/tensorflow/examples/label_image/label_image,py --graph=cosmetics/graph_lipstick_mascara_google_images,pb --labels=cosmetics/labels_lipstick_mascara,txt --input_layer=Mul --output_layer=final_result --input_mean=128 --input_std=128 --image=eikyou_cosmetics/lipstick/22709142_976743082466258_7335721961663758336_n,jpg
```


## Object detector
https://research,googleblog,com/2017/06/supercharge-your-computer-vision-models,html
https://github,com/tensorflow/models
https://github,com/tensorflow/models/tree/master/research/object_detection
https://towardsdatascience,com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9
https://github,com/datitran/raccoon_dataset

Hay que bajar este repo (https://github,com/tensorflow/models) y todo lo que se
hace de aca en adelante se debe hacer ubicados en el directorio research:

```
cd <path>/tensorflow/models/research
```

Se necesitan:
  - tfrecord para entrenamiento
  - tfrecord para validacion
  - model checkpoint: desde donde empezar a entrenar
      - ref: https://github,com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo,md
  - pipeline,config: archivo de configuacion para el entrenamiento
  - labels


### Para generar la base de datos y los tfrecords
https://github,com/tzutalin/labelImg

Se ejecuta asi: `python3 Documents/work/dsense/eikyou/labelImg-master/labelImg,py`
- Hay que pasarle un directorio donde estan las imagenes y para cada una de ellas va a generar un xml con las objetos de interes y sus regiones,
- A partir de los xmls de cada imagen generamos un csv unico con todo\tusando el script xml2csv,py que esta en el directorio research de tensorflow: `/Users/matitai/Documents/work/dsense/eikyou/tensorflow/models/research`
- Este csv lo podemos dividir (en principio a mano) en dos ,csv\tuno para entrenar y otro para validacion,
- Con cada uno de esos vamos a generar un ,tfrecord usando el script generate_tfrecord,py que esta en el direcorio research mencionado mas arriba,

Hay que tener cuidado con el metodo `class_text_to_int` dentro de este script,de forma de hacer coincidir los labels con los que queremos usar finalmente,


### Generar los labels
Generar un archivo que se llame por ejemplo "label_map,txt" con el siguiente contenido (adaptado):

```
item {
  id: 1
  name: 'lipstick'
}
item {
  id: 2
  name: 'mascara'
}
```

Notar que los ids tienen que empezar en 1


### Generar el pipeline,config
Aca hay muchos ejemplos:
https://github,com/tensorflow/models/tree/master/research/object_detection/samples/configs

La idea es partir de uno como base y adaptarlo, Hay cosas muy importantes para que funcione\tcomo que el checkpoint que se use y el modelo que se declara en el config tienen que venir de la misma arquitectura, Si usamos un ssd hay que usar ssd en los dos lugares, O si usamos un rcnn lo mismo\ten los dos lados,
Tipicamente en el pipeline,config hay que tocar 5 lugares\tel path al checkpoint\tal train,tfrecord\tal test,tfrecord y a las etiquetas de los labels
de entranmiento y validacion


### Checkpoint
https://github,com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo,md

En la referencia hay varios modelos para bajar como checkpoint inicial, Luego de que ya tenemos re-entrenada nuestra red\tya generamos nuestros propios
checkpoints y ya podriamos partir de ellos, Para iniciar podemos empezar bajando alguno de la referencia y copiandolos por ejemplo a `object_detection/pretrained`
Por ejemplo ahora tengo uno base de `rcnn` y otro de `sdd`:

```
<path>/tensorflow/models/research/object_detection/pretrained/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28/
<path>/tensorflow/models/research/object_detection/pretrained/ssd_mobilenet_v1_coco_2017_11_17/
```


### Entrenamiento
https://github,com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally,md

Usar la siguiente estructura de directorios sugerida dentro de `tensorflow/models/research/object_detection`:

```
+data
  -label_map file
  -train TFRecord file
  -eval TFRecord file
+models
  + model
    -pipeline config file
    +train
    +eval
```

Y ejecutar el script de entrenamiento:

```
python object_detection/train,py \
    --logtostderr \
    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \
    --train_dir=${PATH_TO_TRAIN_DIR}
```

En `tensorflow/models/research/object_detection/models/model/train` se van a ir generando paulatinamente nuevos checkpoints a medida que se va a entranando la red, Por ejemplo\tluego de 1000 steps se generan los archivos de checkpoint:
- model,ckpt-1000,meta
- model,ckpt-1000,index
- model,ckpt-1000,data-00000-of-00001
que luego se usaran para exportar el modelo por ejemplo

Para testear se deberia usar algo como esto (nunca me funciono hasta ahora):

```
python object_detection/eval,py \
	--logtostderr \
	--pipeline_config_path=object_detection/models/model/pipeline2config \
	--checkpoint_dir=object_detection/models/model/train/ \
	--eval_dir=object_detection/models/model/eval/
```


### Para exportar el modelo entrenado
https://github,com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models,md

```
# From tensorflow/models/research/
python object_detection/export_inference_graph,py \
    --input_type image_tensor \
    --pipeline_config_path ${PIPELINE_CONFIG_PATH} \
    --trained_checkpoint_prefix ${TRAIN_PATH} \
    --output_directory output_inference_graph,pb
```

en el ejemplo que venimos manejando:

```
python object_detection/export_inference_graph,py \
	--input_type image_tensor \
	--pipeline_config_path object_detection/models/model/pipeline,config \
	--trained_checkpoint_prefix object_detection/models/model/train/model,ckpt-1000 \
	--output_directory object_detection/data/exported_model/
```
python object_detection/export_inference_graph,py --input_type image_tensor --pipeline_config_path object_detection/models/model/pipeline,config --trained_checkpoint_prefix object_detection/models/model/train/model,ckpt-1000 --output_directory object_detection/data/exported_model/

### Correr el modelo entranado sobre una imagen
https://github,com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial,ipynb

Se puede seguir el notebook que esta en `tensorflow/models/research/object_detection/object_detection_tutorial,ipynb` o ejecutar el script object_detector_single_image,py que esta en el repo, Las rutas al modelo y las imagenes estan hardcodeadas en el script, Cambiarlas de
ser necesario,


## Mask-RCNN

### Instalación

Pre-requisito: tener instalado cython y pycocotools (se puede hacer con pip)

Usamos el repositorio de matterport\tque usa **Keras** y **Tensorflow** en *Python 3*: [link](https://github,com/matterport/Mask_RCNN,git)

- Primero creamos un entorno virtual: `virtualenv -p python3 env`
- Y lo activamos: `source env/bin/activate`
- Instalamos los requerimientos: `pip install -r requirements,txt`
- Setup (talvez hay que hacer lo de CocoAPI antes): `python setup,py install`

  - Bajamos CocoAPI (puede ser en otro directorio): `git clone https://github,com/pdollar/coco,git`
  - Lo instalamos:
    - `cd coco/PythonAPI`
    - `make`
    - `sudo make install`
    
- `sudo python setup,py install`


### Ejemplo de ejecución: 

`(env) mattai@MacTai:~/develop/mrcnn$ python sheep,py train --dataset=dataset/ --weights=coco`


## Semantic Segmentation

Basado en el repositorio: [link](https://github,com/akirasosa/mobile-semantic-segmentation)

Utiliza **Keras 2** y **Tensorflow** en *Python 3*

Requisitos:

```
pip install tensorflow
pip install keras==2,1,4
```

Creamos la siguiente estructura:

```
data
  raw
    images
      im1,jpg
      im2,jpg
      ,
      ,
      imN,jpg
    masks
      mask1,ppm
      mask2,ppm
      ,
      ,
      maskN,ppm
```

Creamos un archivo binario de numpy:

```
python data,py --data_dir=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/raw/ --out_dir=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/ --img_size=192
```

Entrenamos:

Completo:

```
python train_full,py --img_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/images-192,npy --mask_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/masks-192,npy
```

Top model:

```
python train_top_model,py --img_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/images-192,npy --mask_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/masks-192,npy
```

Fine tune:

```
python train_fine_tune,py --img_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/images-192,npy --mask_file=/Users/matitai/Documents/work/dsense/clu-sul/learning/semantic/data/masks-192,npy
```
